{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Output\n",
    "\n",
    "In this notebook, we explore how to get models to output structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs are often viewed as models that take in text and output text.  \n",
    "Indeed, this is how they are often used in chatbot applications for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hi!\"}],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are often situations where we want to get structured output from LLMs.  \n",
    "For example, you want a model to classify movie review in positive and negative.  \n",
    "Simply asking a model for positive or negative may not get you the desired output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review is negative.\n"
     ]
    }
   ],
   "source": [
    "movie_review = \"\"\"\n",
    "I hated every second of this movie.\n",
    "\"\"\"\n",
    "content = f\"\"\"\n",
    "Here is a movie review:\n",
    "{movie_review}\n",
    "\n",
    "Classify it as positive or negative.\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": content,}],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model responded with a sentence, not just the word \"negative\".  \n",
    "We can try to do prompt engineering to improve the situation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "movie_review = \"\"\"\n",
    "I hated every second of this movie.\n",
    "\"\"\"\n",
    "content = f\"\"\"\n",
    "Here is a movie review:\n",
    "{movie_review}\n",
    "\n",
    "Respond with a single word: `positive` or `negative`.\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": content,}],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that worked, but we can do better.  \n",
    "Note the following:  \n",
    "\n",
    "1. We had to do prompt engineering.  \n",
    "2. The prompt engineering is not very robust.  \n",
    "  - It may fail on the next invocation.  \n",
    "  - It may fail if we switch models.  \n",
    "3. The moment we switch the expected output, we have to prompt engineer again.\n",
    "\n",
    "Clearly, this is not a good solution.  \n",
    "LLM providers have realised this and are now providing structured output APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(value='negative')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Sentiment(BaseModel):\n",
    "    value: Literal[\"positive\", \"negative\"]\n",
    "\n",
    "movie_review = \"\"\"\n",
    "I hated every second of this movie.\n",
    "\"\"\"\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": movie_review},\n",
    "    ],\n",
    "    response_format=Sentiment,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  \n",
    "We could define a structure using Pydantic.  \n",
    "Furthermore, we could feed the `movie_review` directly, circumventing the need for prompt engineering.  \n",
    "The model responded directly with an instance of the Pydantic Sentiment class.  \n",
    "That means we get type checking and validation!  \n",
    "This is clearly a much better developer experience.\n",
    "\n",
    "Note that `client.chat.completions.parse` is a wrapper around `client.chat.completions.create`.  \n",
    "It is equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(value='negative')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explain the Semantic model schema to the model\n",
    "sentiment_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"value\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"positive\", \"negative\"],\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"value\"\n",
    "    ],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "sentiment_format = {\n",
    "    \"name\": \"Sentiment\",\n",
    "    \"description\": \"Sentiment of the movie\",\n",
    "    \"schema\": sentiment_schema,\n",
    "    \"strict\": True\n",
    "}\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": sentiment_format,    \n",
    "}\n",
    "\n",
    "# Invoke the model\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": movie_review},\n",
    "    ],\n",
    "    response_format=response_format,\n",
    ")\n",
    "\n",
    "# Parse the output into a Pydantic model instance\n",
    "Sentiment.model_validate_json(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  \n",
    "The `parse` is a nice abstraction over the `create` method that takes away some complexity for us developers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the other notebooks, we learned about \"tool calling\".  \n",
    "How is \"tool calling\" different from \"structured output\"?  \n",
    "Let's inspect the completion response that we parsed into the pydantic instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"{\\\"value\\\":\\\"negative\\\"}\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.to_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that \"structured output\" does not use any `tool_calls`.  \n",
    "Rather, it puts the output directly in the `content` field.  \n",
    "This is an indication that this works differently from `tool calling`.  \n",
    "OpenAI doesn't release much information on how this works behind the scenes.  \n",
    "But we can expect that there is a special chat template that explains to the model what the expected `response_format` is.  \n",
    "The model is furthermore instructed to output a regular message in json format.  \n",
    "Most likely there is a portion of the training data where the model is specifically trained to do structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have achieved the same thing with \"tool calling\" as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": null,\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": [],\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"id\": \"call_n42xQV1PrQDh4hvhAo8jVJoL\",\n",
      "      \"function\": {\n",
      "        \"arguments\": \"{\\\"value\\\":\\\"negative\\\"}\",\n",
      "        \"name\": \"Sentiment\"\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"Sentiment\",\n",
    "        \"description\": \"Sentiment of the movie\",\n",
    "        \"parameters\": sentiment_schema,\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": movie_review}],\n",
    "    tools=tools\n",
    ")\n",
    "print(completion.choices[0].message.to_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple enough.  \n",
    "So which one should we use?  \n",
    "[OpenAI themselves recommend the following](https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat#function-calling-vs-response-format):  \n",
    "\n",
    "\"\"\"  \n",
    "When to use Structured Outputs via function calling vs via response_format\n",
    "Structured Outputs is available in two forms in the OpenAI API:\n",
    "\n",
    "When using function calling\n",
    "When using a json_schema response format\n",
    "Function calling is useful when you are building an application that bridges the models and functionality of your application.\n",
    "\n",
    "For example, you can give the model access to functions that query a database in order to build an AI assistant that can help users with their orders, or functions that can interact with the UI.\n",
    "\n",
    "Conversely, Structured Outputs via response_format are more suitable when you want to indicate a structured schema for use when the model responds to the user, rather than when the model calls a tool.\n",
    "\n",
    "For example, if you are building a math tutoring application, you might want the assistant to respond to your user using a specific JSON Schema so that you can generate a UI that displays different parts of the model's output in distinct ways.\n",
    "\n",
    "Put simply:\n",
    "\n",
    "If you are connecting the model to tools, functions, data, etc. in your system, then you should use function calling\n",
    "If you want to structure the model's output when it responds to the user, then you should use a structured response_format\n",
    "The remainder of this guide will focus on non-function calling use cases in the Chat Completions API. To learn more about how to use Structured Outputs with function calling, check out the Function Calling guide.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important feature we have not touched upon yet.  \n",
    "So far, we have been asking the model nicely to output json.  \n",
    "As we have seen, this works well for the simple cases we have tested.  \n",
    "However, with very complex schemas and also with less powerful models,  \n",
    "we may run into issues where the model doesn't output a valid json schema.  \n",
    "What can we do in that case?  \n",
    "\n",
    "One good solution is to simplify the expected output schema.  \n",
    "If the model is really struggling, it means that you you are asking it to do too complex of a task.  \n",
    "\n",
    "However, you may run into situations where the business logic requires a complex schema and there is no way around it.  \n",
    "In this case, there is still an option.  \n",
    "The model generates the answer token by token.  \n",
    "We can intercept the output and validate it at each step.  \n",
    "In this way, we can discard any invalid tokens and take the next most likely token.  \n",
    "This means the validation needs to happen inside the server where the model is being invoked,  \n",
    "as we need access to all the logits that the model outputs.  \n",
    "Luckily, OpenAI, vLLM and Ollama now have support for this feature as well as many other providers.  \n",
    "In fact, we have secretly used it already!  \n",
    "Note the parameter `\"strict\": True` in our `tools` and `response_format`.  \n",
    "This parameter indicates to OpenAI that they should discard invalid tokens.  \n",
    "Therefore, this ensures that the response complies with the schema we provided.  \n",
    "\n",
    "With `\"strict\": False`, we are purely relying on the model ability to comply to the schema.  \n",
    "\n",
    "Note also that `\"strict\": True` is like putting a constraint on the model.  \n",
    "So there is some risk that this impacts the model performance.  \n",
    "`\"strict\": True` also means that your schema needs to be supported by OpenAI or you will experience errors.\n",
    "OpenAI themselves now recommend to use `\"strict\": True` always, so the impact is probably negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can do structured output with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(value='negative')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").with_structured_output(\n",
    "    Sentiment,\n",
    "    strict=True, # Or False\n",
    "    method=\"json_schema\", # or \"function_calling\" for the \"tool calling\" approach.\n",
    ")\n",
    "sentiment = model.invoke(movie_review)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can enable \"strict\" \"tool calling\" through `model.bind_tools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Sentiment (call_B8rnKrx7yy0ln3zpW7fmWFxJ)\n",
      " Call ID: call_B8rnKrx7yy0ln3zpW7fmWFxJ\n",
      "  Args:\n",
      "    value: negative\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(\n",
    "    tools=[Sentiment],\n",
    "    strict=True,\n",
    ")\n",
    "message = model.invoke(movie_review)\n",
    "message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Ollama?\n",
    "Ollama also has support for structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(value='negative')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"mistral-small\",\n",
    "    base_url=\"http://10.1.0.255:11434\",\n",
    ").with_structured_output(\n",
    "    Sentiment,\n",
    "    method=\"json_schema\", # or \"function_calling\" for the \"tool calling\" approach.\n",
    ")\n",
    "sentiment = model.invoke(movie_review)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Ollama has no \"strict\" parameter.  \n",
    "Instead, `method=\"json_schema\"` will use `strict=True` and `method=\"function_calling\"` will use `strict=False`.\n",
    "\n",
    "The `function_calling` doesn't work very well, because the model may respond with a regular message without using the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It sounds like you didn't enjoy the movie. Is there anything specific you disliked about it?\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"mistral-small\",\n",
    "    base_url=\"http://10.1.0.255:11434\",\n",
    ").with_structured_output(\n",
    "    Sentiment,\n",
    "    method=\"function_calling\",\n",
    "    include_raw=True\n",
    ")\n",
    "raw = model.invoke(movie_review)\n",
    "raw[\"raw\"].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is thus recommended to use `method=\"json_schema\"` for structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example with `method=\"json_schema\"` seemed to work pretty well.  \n",
    "But there is a crucial flaw!  \n",
    "\n",
    "It is important to know that OpenAI does pass the response_format to the model.  \n",
    "If you use vLLM or Ollama, the response_format is **not passed** to the model!  \n",
    "\n",
    "Thus, with Ollama, the model never received the schema of the `Sentiment` pydantic class.  \n",
    "Why did it work then? Because Ollama used `strict=True` to discard any invalid tokens from the model.  \n",
    "This will not work for more complex cases where the model needs to understand the structure that it needs to output.  \n",
    "\n",
    "Let's test this out.\n",
    "We will create a confusing class that can only be parsed correclty if the model has access to the response_format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ConfusingClass(BaseModel):\n",
    "    integer: int = Field(description=\"The sum of the two integers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfusingClass(integer=3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\").with_structured_output(\n",
    "    ConfusingClass,\n",
    "    strict=True\n",
    ")\n",
    "confusing_class = model.invoke(\"1,2\")\n",
    "confusing_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpt-4o-mini got it right! It means that it had access to the `description`.  \n",
    "\n",
    "Now what about mistral-small with Ollama?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfusingClass(integer=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"mistral-small\",\n",
    "    base_url=\"http://10.1.0.255:11434\",\n",
    ").with_structured_output(\n",
    "    ConfusingClass,\n",
    "    method=\"json_schema\",\n",
    ")\n",
    "confusing_class = model.invoke(\"1,2\")\n",
    "confusing_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can't do it.  \n",
    "The reason is that it did not receive the `description` from the response format.  \n",
    "To fix this, we have to put the schema in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfusingClass(integer=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"mistral-small\",\n",
    "    base_url=\"http://10.1.0.255:11434\",\n",
    ").with_structured_output(\n",
    "    ConfusingClass,\n",
    "    method=\"json_schema\",\n",
    ")\n",
    "\n",
    "user_message = \"1,2\"\n",
    "prompt = f\"\"\"{user_message}\n",
    "\n",
    "Respond with a json in the following schema:\n",
    "{json.dumps(ConfusingClass.model_json_schema(), indent=2)}\"\"\"\n",
    "\n",
    "confusing_class = model.invoke(prompt)\n",
    "confusing_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it got it correctly!  \n",
    "Be careful about this point.  \n",
    "You cannot exchange `ChatOpenAI` with `ChatOllama` freely if you are using structured output.  \n",
    "\n",
    "To make it extra confusing, Ollama does pass the schema if you use `function_calling`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfusingClass(integer=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"mistral-small\",\n",
    "    base_url=\"http://10.1.0.255:11434\",\n",
    ").with_structured_output(\n",
    "    ConfusingClass,\n",
    "    method=\"function_calling\",\n",
    ")\n",
    "confusing_class = model.invoke(\"1,2\")\n",
    "confusing_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be very careful when using structured output to ensure that the model receives the full set of instructions needed to complete the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
